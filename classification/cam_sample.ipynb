{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.dropout import Dropout\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "import timm\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'IMG_WIDTH':720,\n",
    "    'IMG_HEIGTH':1280,\n",
    "    'EPOCHS':3,\n",
    "    'LEARNING_RATE':3e-4,\n",
    "    'BATCH_SIZE':8,\n",
    "    'SEED':3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.init(\n",
    "#     project=\"Final Project\", \n",
    "#     entity=\"aitech4_cv3\",\n",
    "#     name='classification_resnet50',\n",
    "#     config = {\n",
    "#         \"lr\" : CFG['LEARNING_RATE'],\n",
    "#         \"epoch\" : CFG['EPOCHS'],\n",
    "#         \"batch_size\" : CFG['BATCH_SIZE'],\n",
    "#     }\n",
    "#     )\n",
    "\n",
    "# config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = pd.read_csv('/opt/ml/data_csv/data.csv')\n",
    "label = data_file['info']\n",
    "data =  data_file.drop(['info',\"Unnamed: 0\"], axis=1)\n",
    "X_train, y_train, X_test, y_test = train_test_split(data, label, test_size=0.2, random_state=CFG['SEED'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose([A.Resize(CFG['IMG_HEIGTH'],CFG['IMG_WIDTH']),\n",
    "                            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
    "                            ToTensorV2()\n",
    "                            ])\n",
    "\n",
    "\n",
    "val_transform = A.Compose([\n",
    "                            A.Resize(CFG['IMG_HEIGTH'],CFG['IMG_WIDTH']),\n",
    "                            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
    "                            ToTensorV2()\n",
    "                            ])\n",
    "\n",
    "\n",
    "\n",
    "test_transform = A.Compose([\n",
    "                            A.Resize(CFG['IMG_HEIGTH'],CFG['IMG_WIDTH']),\n",
    "                            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
    "                            ToTensorV2()\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reset_index()\n",
    "X_test = X_test.reset_index()\n",
    "y_train = y_train.reset_index()\n",
    "y_test = y_test.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_paths, labels, transforms=None):\n",
    "        self.img_paths = img_paths['filepath']\n",
    "        self.labels = labels['info']\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.img_paths[index]\n",
    "\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image=image)['image']\n",
    "            \n",
    "        \n",
    "        if self.labels is not None:\n",
    "            label = self.labels[index]\n",
    "            return image, label\n",
    "        else:\n",
    "            return image\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(X_train, X_test, train_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataset = CustomDataset(y_train, y_test, val_transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.backbone = timm.create_model(model_name='efficientnet_b0', pretrained=True)\n",
    "      \n",
    "        self.conv_stem = self.backbone.conv_stem\n",
    "        self.bn1 = self.backbone.bn1\n",
    "        self.act1 = self.backbone.act1\n",
    "        self.init_block =  nn.Sequential(self.conv_stem, self.bn1, self.act1)\n",
    "\n",
    "        self.blocks = self.backbone.blocks[0:7]\n",
    "\n",
    "        self.conv_head = self.backbone.conv_head\n",
    "        self.bn2 = self.backbone.bn2\n",
    "        self.act2 = self.backbone.act2\n",
    "        self.global_pool = self.backbone.global_pool\n",
    "\n",
    "        self.classifier = nn.Linear(1280,num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_stem(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act1(x)\n",
    "\n",
    "        x = self.blocks(x)\n",
    "\n",
    "        x = self.conv_head(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.act2(x)\n",
    "        cam_feature = nn.ReLU()(x) \n",
    "\n",
    "        x = self.global_pool(x)\n",
    "        x = self.classifier(x)\n",
    "        x = nn.Sigmoid()(x)\n",
    "        return x, cam_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_pred, y):\n",
    "    top_pred = torch.round(y_pred)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EfficientNet(\n",
      "  (conv_stem): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (act1): SiLU(inplace=True)\n",
      "  (blocks): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): DepthwiseSeparableConv(\n",
      "        (conv_dw): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act1): SiLU(inplace=True)\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act1): SiLU(inplace=True)\n",
      "          (conv_expand): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (gate): Sigmoid()\n",
      "        )\n",
      "        (conv_pw): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act2): Identity()\n",
      "      )\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): InvertedResidual(\n",
      "        (conv_pw): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act1): SiLU(inplace=True)\n",
      "        (conv_dw): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "        (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act2): SiLU(inplace=True)\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act1): SiLU(inplace=True)\n",
      "          (conv_expand): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (gate): Sigmoid()\n",
      "        )\n",
      "        (conv_pwl): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): InvertedResidual(\n",
      "        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act1): SiLU(inplace=True)\n",
      "        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act2): SiLU(inplace=True)\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act1): SiLU(inplace=True)\n",
      "          (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (gate): Sigmoid()\n",
      "        )\n",
      "        (conv_pwl): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): InvertedResidual(\n",
      "        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act1): SiLU(inplace=True)\n",
      "        (conv_dw): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
      "        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act2): SiLU(inplace=True)\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act1): SiLU(inplace=True)\n",
      "          (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (gate): Sigmoid()\n",
      "        )\n",
      "        (conv_pwl): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): InvertedResidual(\n",
      "        (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act1): SiLU(inplace=True)\n",
      "        (conv_dw): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act2): SiLU(inplace=True)\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act1): SiLU(inplace=True)\n",
      "          (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (gate): Sigmoid()\n",
      "        )\n",
      "        (conv_pwl): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): InvertedResidual(\n",
      "        (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act1): SiLU(inplace=True)\n",
      "        (conv_dw): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
      "        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act2): SiLU(inplace=True)\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act1): SiLU(inplace=True)\n",
      "          (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (gate): Sigmoid()\n",
      "        )\n",
      "        (conv_pwl): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): InvertedResidual(\n",
      "        (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act1): SiLU(inplace=True)\n",
      "        (conv_dw): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "        (bn2): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act2): SiLU(inplace=True)\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act1): SiLU(inplace=True)\n",
      "          (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (gate): Sigmoid()\n",
      "        )\n",
      "        (conv_pwl): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): InvertedResidual(\n",
      "        (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act1): SiLU(inplace=True)\n",
      "        (conv_dw): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "        (bn2): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act2): SiLU(inplace=True)\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act1): SiLU(inplace=True)\n",
      "          (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (gate): Sigmoid()\n",
      "        )\n",
      "        (conv_pwl): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): InvertedResidual(\n",
      "        (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act1): SiLU(inplace=True)\n",
      "        (conv_dw): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
      "        (bn2): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act2): SiLU(inplace=True)\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act1): SiLU(inplace=True)\n",
      "          (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (gate): Sigmoid()\n",
      "        )\n",
      "        (conv_pwl): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): InvertedResidual(\n",
      "        (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act1): SiLU(inplace=True)\n",
      "        (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
      "        (bn2): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act2): SiLU(inplace=True)\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act1): SiLU(inplace=True)\n",
      "          (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (gate): Sigmoid()\n",
      "        )\n",
      "        (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): InvertedResidual(\n",
      "        (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act1): SiLU(inplace=True)\n",
      "        (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
      "        (bn2): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act2): SiLU(inplace=True)\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act1): SiLU(inplace=True)\n",
      "          (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (gate): Sigmoid()\n",
      "        )\n",
      "        (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): InvertedResidual(\n",
      "        (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act1): SiLU(inplace=True)\n",
      "        (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
      "        (bn2): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act2): SiLU(inplace=True)\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act1): SiLU(inplace=True)\n",
      "          (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (gate): Sigmoid()\n",
      "        )\n",
      "        (conv_pwl): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): InvertedResidual(\n",
      "        (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act1): SiLU(inplace=True)\n",
      "        (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "        (bn2): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act2): SiLU(inplace=True)\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act1): SiLU(inplace=True)\n",
      "          (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (gate): Sigmoid()\n",
      "        )\n",
      "        (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): InvertedResidual(\n",
      "        (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act1): SiLU(inplace=True)\n",
      "        (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "        (bn2): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act2): SiLU(inplace=True)\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act1): SiLU(inplace=True)\n",
      "          (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (gate): Sigmoid()\n",
      "        )\n",
      "        (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (3): InvertedResidual(\n",
      "        (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act1): SiLU(inplace=True)\n",
      "        (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "        (bn2): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act2): SiLU(inplace=True)\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act1): SiLU(inplace=True)\n",
      "          (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (gate): Sigmoid()\n",
      "        )\n",
      "        (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): InvertedResidual(\n",
      "        (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act1): SiLU(inplace=True)\n",
      "        (conv_dw): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
      "        (bn2): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act2): SiLU(inplace=True)\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act1): SiLU(inplace=True)\n",
      "          (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (gate): Sigmoid()\n",
      "        )\n",
      "        (conv_pwl): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv_head): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (bn2): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (act2): SiLU(inplace=True)\n",
      "  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
      "  (classifier): Linear(in_features=1280, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = timm.create_model(model_name='efficientnet_b0', pretrained=True)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, criterion, test_loader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    model_preds = []\n",
    "    true_labels = []\n",
    "    \n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img, label in iter(test_loader):\n",
    "            img, label = img.float().to(device), label.float().to(device).reshape(-1,1)\n",
    "            \n",
    "            model_pred,_ = model(img)\n",
    "\n",
    "            loss = criterion(model_pred, label)\n",
    "            \n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            val_acc.append(calculate_accuracy(model_pred, label).item())\n",
    "            \n",
    "            true_labels += label.detach().cpu().numpy().tolist()\n",
    "\n",
    "    return np.mean(val_loss), np.mean(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_of_epoch(start, end):\n",
    "    time_per_epoch = end - start\n",
    "    time_per_epoch_min = int(time_per_epoch / 60)\n",
    "    time_per_epoch_sec = int(time_per_epoch -(time_per_epoch_min*60))\n",
    "    return time_per_epoch_min, time_per_epoch_sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, test_loader, scheduler, device):\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "\n",
    "    best_score = 0\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(1, CFG[\"EPOCHS\"]+1):\n",
    "        model.train()\n",
    "        start_time = time.monotonic()\n",
    "        train_loss = []\n",
    "        epoch_acc = 0\n",
    "        for step,(img, label) in enumerate(train_loader): #tqdm(iter(train_loader)\n",
    "\n",
    "            img, label = img.float().to(device), label.float().to(device)\n",
    "            optimizer.zero_grad()\n",
    "            label = label.view(-1,1)\n",
    "\n",
    "            model_pred,_ = model(img)\n",
    "\n",
    "            acc = calculate_accuracy(model_pred, label)\n",
    "\n",
    "            loss = criterion(model_pred, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "            if (step + 1) % 5 == 0:\n",
    "                print(f'Epoch [{epoch}], Step [{step+1}], Train Loss : [{round(loss.item(),4):.5f}] Train acc : [{acc:.5f}]')\n",
    "\n",
    "        end_time = time.monotonic()\n",
    "        epoch_min, epoch_sec = time_of_epoch(start_time, end_time)\n",
    "        train_loss_m = np.mean(train_loss)\n",
    "        epoch_acc += acc.item()\n",
    "        val_loss, val_acc = validation(model, criterion, test_loader, device)\n",
    "\n",
    "        print(f'Epoch [{epoch}], Train Loss : [{train_loss_m:.5f}] Val Loss : [{val_loss:.5f}] Val acc : [{val_acc:.5f}],Time : {epoch_min}m {epoch_sec}s')\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "            \n",
    "        if best_score < val_acc:\n",
    "            best_model = model\n",
    "            best_score = val_acc\n",
    "            print(f\"save_best_pth EPOCH {epoch}\")\n",
    "            torch.save(model.state_dict(),\"../../model_save_dir/best_cam.pth\")\n",
    "        \n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Step [5], Train Loss : [0.67370] Train acc : [0.75000]\n",
      "Epoch [1], Step [10], Train Loss : [0.48200] Train acc : [0.87500]\n",
      "Epoch [1], Step [15], Train Loss : [0.42420] Train acc : [0.87500]\n",
      "Epoch [1], Step [20], Train Loss : [0.53040] Train acc : [0.62500]\n",
      "Epoch [1], Step [25], Train Loss : [0.56030] Train acc : [0.62500]\n",
      "Epoch [1], Step [30], Train Loss : [0.30840] Train acc : [1.00000]\n",
      "Epoch [1], Step [35], Train Loss : [0.45700] Train acc : [0.62500]\n",
      "Epoch [1], Step [40], Train Loss : [0.66150] Train acc : [0.62500]\n",
      "Epoch [1], Step [45], Train Loss : [0.63550] Train acc : [0.75000]\n",
      "Epoch [1], Step [50], Train Loss : [0.26390] Train acc : [1.00000]\n",
      "Epoch [1], Step [55], Train Loss : [0.45530] Train acc : [0.75000]\n",
      "Epoch [1], Step [60], Train Loss : [0.38190] Train acc : [0.87500]\n",
      "Epoch [1], Step [65], Train Loss : [0.22120] Train acc : [1.00000]\n",
      "Epoch [1], Step [70], Train Loss : [0.51740] Train acc : [0.75000]\n",
      "Epoch [1], Step [75], Train Loss : [0.79730] Train acc : [0.62500]\n",
      "Epoch [1], Step [80], Train Loss : [0.32060] Train acc : [0.87500]\n",
      "Epoch [1], Step [85], Train Loss : [0.20600] Train acc : [0.87500]\n",
      "Epoch [1], Step [90], Train Loss : [0.45790] Train acc : [0.87500]\n",
      "Epoch [1], Step [95], Train Loss : [0.29570] Train acc : [0.87500]\n",
      "Epoch [1], Step [100], Train Loss : [0.36370] Train acc : [0.75000]\n",
      "Epoch [1], Step [105], Train Loss : [0.12420] Train acc : [1.00000]\n",
      "Epoch [1], Step [110], Train Loss : [0.22990] Train acc : [1.00000]\n",
      "Epoch [1], Step [115], Train Loss : [0.61460] Train acc : [0.75000]\n",
      "Epoch [1], Step [120], Train Loss : [0.08830] Train acc : [1.00000]\n",
      "Epoch [1], Step [125], Train Loss : [0.30350] Train acc : [0.87500]\n",
      "Epoch [1], Step [130], Train Loss : [0.52740] Train acc : [0.75000]\n",
      "Epoch [1], Step [135], Train Loss : [0.11970] Train acc : [1.00000]\n",
      "Epoch [1], Step [140], Train Loss : [0.28110] Train acc : [0.87500]\n",
      "Epoch [1], Step [145], Train Loss : [0.14940] Train acc : [1.00000]\n",
      "Epoch [1], Step [150], Train Loss : [0.32660] Train acc : [0.87500]\n",
      "Epoch [1], Step [155], Train Loss : [0.20770] Train acc : [0.87500]\n",
      "Epoch [1], Step [160], Train Loss : [0.38450] Train acc : [0.75000]\n",
      "Epoch [1], Step [165], Train Loss : [0.23910] Train acc : [0.87500]\n",
      "Epoch [1], Step [170], Train Loss : [0.44800] Train acc : [0.75000]\n",
      "Epoch [1], Step [175], Train Loss : [0.10860] Train acc : [1.00000]\n",
      "Epoch [1], Step [180], Train Loss : [0.55950] Train acc : [0.62500]\n",
      "Epoch [1], Step [185], Train Loss : [0.36140] Train acc : [0.75000]\n",
      "Epoch [1], Step [190], Train Loss : [0.35880] Train acc : [0.75000]\n",
      "Epoch [1], Train Loss : [0.41208] Val Loss : [0.35617] Val acc : [0.84003],Time : 1m 55s\n",
      "save_best_pth EPOCH 1\n",
      "Epoch [2], Step [5], Train Loss : [0.48930] Train acc : [0.75000]\n",
      "Epoch [2], Step [10], Train Loss : [0.19650] Train acc : [1.00000]\n",
      "Epoch [2], Step [15], Train Loss : [0.15630] Train acc : [1.00000]\n",
      "Epoch [2], Step [20], Train Loss : [0.45790] Train acc : [0.75000]\n",
      "Epoch [2], Step [25], Train Loss : [0.65840] Train acc : [0.75000]\n",
      "Epoch [2], Step [30], Train Loss : [0.25030] Train acc : [0.87500]\n",
      "Epoch [2], Step [35], Train Loss : [0.73050] Train acc : [0.75000]\n",
      "Epoch [2], Step [40], Train Loss : [0.27580] Train acc : [0.75000]\n",
      "Epoch [2], Step [45], Train Loss : [0.52210] Train acc : [0.87500]\n",
      "Epoch [2], Step [50], Train Loss : [0.15120] Train acc : [0.87500]\n",
      "Epoch [2], Step [55], Train Loss : [0.40100] Train acc : [0.75000]\n",
      "Epoch [2], Step [60], Train Loss : [0.43950] Train acc : [0.50000]\n",
      "Epoch [2], Step [65], Train Loss : [0.43320] Train acc : [0.87500]\n",
      "Epoch [2], Step [70], Train Loss : [0.37020] Train acc : [0.87500]\n",
      "Epoch [2], Step [75], Train Loss : [0.04730] Train acc : [1.00000]\n",
      "Epoch [2], Step [80], Train Loss : [0.28310] Train acc : [0.87500]\n",
      "Epoch [2], Step [85], Train Loss : [0.12020] Train acc : [1.00000]\n",
      "Epoch [2], Step [90], Train Loss : [0.98840] Train acc : [0.50000]\n",
      "Epoch [2], Step [95], Train Loss : [0.22350] Train acc : [0.87500]\n",
      "Epoch [2], Step [100], Train Loss : [0.45410] Train acc : [0.87500]\n",
      "Epoch [2], Step [105], Train Loss : [0.10450] Train acc : [1.00000]\n",
      "Epoch [2], Step [110], Train Loss : [0.32570] Train acc : [0.75000]\n",
      "Epoch [2], Step [115], Train Loss : [0.44500] Train acc : [0.75000]\n",
      "Epoch [2], Step [120], Train Loss : [0.19260] Train acc : [0.87500]\n",
      "Epoch [2], Step [125], Train Loss : [0.08400] Train acc : [1.00000]\n",
      "Epoch [2], Step [130], Train Loss : [0.29820] Train acc : [0.75000]\n",
      "Epoch [2], Step [135], Train Loss : [0.53330] Train acc : [0.62500]\n",
      "Epoch [2], Step [140], Train Loss : [0.14060] Train acc : [1.00000]\n",
      "Epoch [2], Step [145], Train Loss : [0.19480] Train acc : [0.87500]\n",
      "Epoch [2], Step [150], Train Loss : [0.16830] Train acc : [0.87500]\n",
      "Epoch [2], Step [155], Train Loss : [0.26710] Train acc : [0.87500]\n",
      "Epoch [2], Step [160], Train Loss : [0.10100] Train acc : [1.00000]\n",
      "Epoch [2], Step [165], Train Loss : [0.31640] Train acc : [0.87500]\n",
      "Epoch [2], Step [170], Train Loss : [0.33660] Train acc : [0.87500]\n",
      "Epoch [2], Step [175], Train Loss : [0.18690] Train acc : [0.87500]\n",
      "Epoch [2], Step [180], Train Loss : [0.09110] Train acc : [1.00000]\n",
      "Epoch [2], Step [185], Train Loss : [0.12870] Train acc : [0.87500]\n",
      "Epoch [2], Step [190], Train Loss : [0.63170] Train acc : [0.75000]\n",
      "Epoch [2], Train Loss : [0.27879] Val Loss : [0.20007] Val acc : [0.91406],Time : 1m 54s\n",
      "save_best_pth EPOCH 2\n",
      "Epoch [3], Step [5], Train Loss : [0.06920] Train acc : [1.00000]\n",
      "Epoch [3], Step [10], Train Loss : [0.06060] Train acc : [1.00000]\n",
      "Epoch [3], Step [15], Train Loss : [0.16210] Train acc : [0.87500]\n",
      "Epoch [3], Step [20], Train Loss : [0.06570] Train acc : [1.00000]\n",
      "Epoch [3], Step [25], Train Loss : [0.25740] Train acc : [0.87500]\n",
      "Epoch [3], Step [30], Train Loss : [0.01780] Train acc : [1.00000]\n",
      "Epoch [3], Step [35], Train Loss : [0.02560] Train acc : [1.00000]\n",
      "Epoch [3], Step [40], Train Loss : [0.01400] Train acc : [1.00000]\n",
      "Epoch [3], Step [45], Train Loss : [0.02480] Train acc : [1.00000]\n",
      "Epoch [3], Step [50], Train Loss : [0.20490] Train acc : [0.87500]\n",
      "Epoch [3], Step [55], Train Loss : [0.19020] Train acc : [0.87500]\n",
      "Epoch [3], Step [60], Train Loss : [0.01460] Train acc : [1.00000]\n",
      "Epoch [3], Step [65], Train Loss : [0.07120] Train acc : [1.00000]\n",
      "Epoch [3], Step [70], Train Loss : [0.57150] Train acc : [0.75000]\n",
      "Epoch [3], Step [75], Train Loss : [0.08140] Train acc : [1.00000]\n",
      "Epoch [3], Step [80], Train Loss : [0.02600] Train acc : [1.00000]\n",
      "Epoch [3], Step [85], Train Loss : [0.17650] Train acc : [1.00000]\n",
      "Epoch [3], Step [90], Train Loss : [0.10950] Train acc : [1.00000]\n",
      "Epoch [3], Step [95], Train Loss : [0.04640] Train acc : [1.00000]\n",
      "Epoch [3], Step [100], Train Loss : [0.03760] Train acc : [1.00000]\n",
      "Epoch [3], Step [105], Train Loss : [0.41230] Train acc : [0.87500]\n",
      "Epoch [3], Step [110], Train Loss : [0.02770] Train acc : [1.00000]\n",
      "Epoch [3], Step [115], Train Loss : [0.11240] Train acc : [1.00000]\n",
      "Epoch [3], Step [120], Train Loss : [0.06220] Train acc : [1.00000]\n",
      "Epoch [3], Step [125], Train Loss : [0.01830] Train acc : [1.00000]\n",
      "Epoch [3], Step [130], Train Loss : [0.13370] Train acc : [0.87500]\n",
      "Epoch [3], Step [135], Train Loss : [0.07260] Train acc : [1.00000]\n",
      "Epoch [3], Step [140], Train Loss : [0.07930] Train acc : [1.00000]\n",
      "Epoch [3], Step [145], Train Loss : [0.05660] Train acc : [1.00000]\n",
      "Epoch [3], Step [150], Train Loss : [0.03430] Train acc : [1.00000]\n",
      "Epoch [3], Step [155], Train Loss : [0.08330] Train acc : [1.00000]\n",
      "Epoch [3], Step [160], Train Loss : [0.04800] Train acc : [1.00000]\n",
      "Epoch [3], Step [165], Train Loss : [0.09330] Train acc : [1.00000]\n",
      "Epoch [3], Step [170], Train Loss : [0.05870] Train acc : [1.00000]\n",
      "Epoch [3], Step [175], Train Loss : [0.02350] Train acc : [1.00000]\n",
      "Epoch [3], Step [180], Train Loss : [0.18070] Train acc : [0.87500]\n",
      "Epoch [3], Step [185], Train Loss : [0.33640] Train acc : [0.87500]\n",
      "Epoch [3], Step [190], Train Loss : [0.10680] Train acc : [1.00000]\n",
      "Epoch [3], Train Loss : [0.15904] Val Loss : [0.20485] Val acc : [0.92448],Time : 1m 54s\n",
      "save_best_pth EPOCH 3\n"
     ]
    }
   ],
   "source": [
    "model = BaseModel()\n",
    "model.eval()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "scheduler = None\n",
    "infer_model = train(model, optimizer, train_loader, val_loader, scheduler, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_paths = \"/opt/ml/img_data/pothole/V0F_HY_0377_20210107_154114_N_CH0_Busan_Sun_Highway_Day_59153.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTestDataset(Dataset):\n",
    "    def __init__(self, img_paths, labels, transforms=None):\n",
    "        self.img_paths = img_paths\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.img_paths\n",
    "\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image=image)['image']\n",
    "        return image\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomTestDataset(test_img_paths, None, test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, img_path, device):\n",
    "    model = BaseModel()\n",
    "    model.load_state_dict(torch.load(\"/opt/ml/model_save_dir/best_cam.pth\", map_location=device))\n",
    "    model = model.to(device)\n",
    "    \n",
    "    model_preds = []\n",
    "    img_paths = []\n",
    "    img_paths.append(img_path)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for img in iter(test_loader):\n",
    "            img = img.float().to(device)\n",
    "            model_pred, c_map = model(img)\n",
    "            model_preds.append(torch.round(model_pred).detach().cpu().numpy())\n",
    "    c_map = nn.ReLU()(c_map)\n",
    "    c_map_w = nn.AdaptiveAvgPool2d(output_size=(1,1))(c_map)\n",
    "    print(c_map_w.shape)\n",
    "    print('Done.')\n",
    "    return model_preds[0][0], c_map, c_map_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imwrite(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    cv2.imwrite(\"/opt/ml/cam_dir/c_0.png\",np.transpose(img, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1280, 1, 1])\n",
      "Done.\n",
      "[[[0.0000000e+00 0.0000000e+00 2.4601144e-01 ... 0.0000000e+00\n",
      "   0.0000000e+00 0.0000000e+00]\n",
      "  [1.8373414e+00 1.4023100e+00 9.0251476e-01 ... 2.6767367e-02\n",
      "   0.0000000e+00 0.0000000e+00]\n",
      "  [3.5191555e+00 2.3378236e+00 1.1276944e+00 ... 5.4869664e-01\n",
      "   1.4029621e+00 0.0000000e+00]\n",
      "  ...\n",
      "  [0.0000000e+00 6.1183429e-01 0.0000000e+00 ... 0.0000000e+00\n",
      "   0.0000000e+00 0.0000000e+00]\n",
      "  [0.0000000e+00 7.2381264e-01 2.1132527e-01 ... 0.0000000e+00\n",
      "   0.0000000e+00 0.0000000e+00]\n",
      "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "   0.0000000e+00 0.0000000e+00]]\n",
      "\n",
      " [[0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "   0.0000000e+00 0.0000000e+00]\n",
      "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "   0.0000000e+00 0.0000000e+00]\n",
      "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "   0.0000000e+00 0.0000000e+00]\n",
      "  ...\n",
      "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "   0.0000000e+00 0.0000000e+00]\n",
      "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "   0.0000000e+00 0.0000000e+00]\n",
      "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "   0.0000000e+00 0.0000000e+00]]\n",
      "\n",
      " [[0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "   0.0000000e+00 0.0000000e+00]\n",
      "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "   0.0000000e+00 0.0000000e+00]\n",
      "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 1.1742149e-03\n",
      "   0.0000000e+00 0.0000000e+00]\n",
      "  ...\n",
      "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "   0.0000000e+00 0.0000000e+00]\n",
      "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "   0.0000000e+00 0.0000000e+00]\n",
      "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "   0.0000000e+00 0.0000000e+00]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "   0.0000000e+00 0.0000000e+00]\n",
      "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "   0.0000000e+00 0.0000000e+00]\n",
      "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "   0.0000000e+00 0.0000000e+00]\n",
      "  ...\n",
      "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "   0.0000000e+00 0.0000000e+00]\n",
      "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "   0.0000000e+00 0.0000000e+00]\n",
      "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "   0.0000000e+00 0.0000000e+00]]\n",
      "\n",
      " [[0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "   8.3863825e-01 0.0000000e+00]\n",
      "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "   9.0445703e-01 0.0000000e+00]\n",
      "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 8.1619394e-01\n",
      "   3.6469838e-01 0.0000000e+00]\n",
      "  ...\n",
      "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "   0.0000000e+00 0.0000000e+00]\n",
      "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "   0.0000000e+00 0.0000000e+00]\n",
      "  [1.4676915e-01 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "   0.0000000e+00 0.0000000e+00]]\n",
      "\n",
      " [[0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "   0.0000000e+00 0.0000000e+00]\n",
      "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "   0.0000000e+00 0.0000000e+00]\n",
      "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "   0.0000000e+00 0.0000000e+00]\n",
      "  ...\n",
      "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "   0.0000000e+00 0.0000000e+00]\n",
      "  [5.5698681e-01 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "   0.0000000e+00 0.0000000e+00]\n",
      "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "   0.0000000e+00 0.0000000e+00]]]\n",
      "(1280, 1, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds, c_map, c_map_w = inference(infer_model, test_img_paths, device)\n",
    "c_map_np = c_map.detach().cpu().numpy()\n",
    "c_map_w = c_map_w.detach().cpu().numpy()\n",
    "\n",
    "print(c_map_np[0])\n",
    "print(c_map_w[0].shape)\n",
    "# print(c_map_np[0])\n",
    "# print(c_map_w[0])\n",
    "\n",
    "cam_result = c_map_np[0]*c_map_w[0]\n",
    "cam_result = torch.tensor(cam_result)\n",
    "\n",
    "cam_result = torch.sum(cam_result, axis=0)\n",
    "cam_result = cam_result.detach().cpu().numpy()\n",
    "c_feature_resize = cv2.resize(cam_result,(1280,720),interpolation=cv2.INTER_LINEAR)\n",
    "cv2.imwrite(\"/opt/ml/cam_dir/c_0.png\",c_feature_resize)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
